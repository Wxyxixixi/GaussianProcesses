{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP():\n",
    "    def __init__(self, kernel, optimizer):\n",
    "        self.kernel = kernel\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def fit(self, X, y, eval_gradient=True):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    def sample_y(self, X, n_samples):\n",
    "        pass\n",
    "    def log_marginal_likelihood(self, theta=None, eval_gradient=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF():\n",
    "    \n",
    "    def __init__(self, length_scale, signal_variance=1, noise_variance=1e-10):\n",
    "        self.length_scale = length_scale\n",
    "        self.signal_variance = signal_variance\n",
    "        self.noise_variance = noise_variance\n",
    "    \n",
    "    def __call__(self, X1, X2=None, eval_gradient=False):\n",
    "        if X2 is None:\n",
    "            # Upper triangular pair-wise distances\n",
    "            dists = pdist(X1, 'sqeuclidean')\n",
    "            K_ = np.exp(-dists / (2 * self.length_scale ** 2))\n",
    "            K_ = squareform(K_)  # copy upper-triangle to lower-triangle\n",
    "            np.fill_diagonal(K_, 1)\n",
    "        else:\n",
    "            if eval_gradient:\n",
    "                raise ValueError(\n",
    "                    \"Should only be evaluating gradient when X2 is None.\")\n",
    "            dists = cdist(X1, X2, metric='sqeuclidean')\n",
    "            K_ = np.exp(-dists / (2 * self.length_scale ** 2))\n",
    "\n",
    "        # Build total K\n",
    "        K = self.signal_variance * K_\n",
    "        K[np.diag_indices_from(K)] += self.noise_variance\n",
    "        \n",
    "        if eval_gradient:\n",
    "            K_grad = np.zeros((K.shape[0], K.shape[1], 3))\n",
    "            # w.r.t length_scale\n",
    "            K_grad[:, :, 0] = self.signal_variance * K_ * squareform(dists) / self.length_scale ** 3\n",
    "            K_grad[:, :, 1] = K_  # w.r.t signal_variance\n",
    "            K_grad[:, :, 2] = np.ones((K.shape))  # w.r.t noise_variance\n",
    "            return K, K_grad\n",
    "        else:\n",
    "            return K\n",
    "        \n",
    "    def set_theta(self, length_scale, signal_variance, noise_variance):\n",
    "        self.length_scale = length_scale\n",
    "        self.signal_variance = signal_variance\n",
    "        self.noise_variance = noise_variance\n",
    "        \n",
    "    def theta(self):\n",
    "        return [self.length_scale, self.signal_variance, self.noise_variance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, eval_gradient=True):\n",
    "    \n",
    "    self.X_train_ = np.copy(X)\n",
    "    self.y_train_ = np.copy(y)\n",
    "    self.kernel_ = copy.copy(self.kernel)  # same as prior kernel but with parameters to be optimized\n",
    "    \n",
    "    # compute alpha of eqn. 2.25, needed for making predictions and computing lml\n",
    "    K = self.kernel_(X, X)\n",
    "    try:\n",
    "        self.L_ = np.linalg.cholesky(K)\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        e.args = (\"The kernel, %s, is not returning a\" \n",
    "                  \"positive definite matrix. Try gradually \"\n",
    "                 \"increasing the 'alpha' parameter of your \"\n",
    "                    \"GaussianProcessRegressor estimator.\"\n",
    "                  % self.kernel_,) + e.args\n",
    "        raise\n",
    "        \n",
    "    self.alpha_ = np.linalg.solve(self.L_.T, np.linalg.solve(self.L_, y))\n",
    "    \n",
    "    def obj_func(theta, eval_gradient):\n",
    "        if eval_gradient:\n",
    "            lml, grad = self.log_marginal_likelihood(theta, eval_gradient=True)\n",
    "            return -lml, -grad\n",
    "        else:\n",
    "            return -self.log_marginal_likelihood(theta)\n",
    "            \n",
    "    results = minimize(obj_func, self.kernel_.theta(), \n",
    "                                      args=(eval_gradient), method=self.optimizer, jac=eval_gradient)\n",
    "    self.log_marginal_likelihood_value = results['fun']\n",
    "    self.kernel_.theta = results['x']\n",
    "    return self\n",
    "GP.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\" Returns predictive mean and variance.\"\"\"\n",
    "    if not hasattr(self, \"X_train_\"):  # Unfitted; predict based on GP prior.\n",
    "        y_mean = np.zeros(X.shape[0])\n",
    "        y_cov = self.kernel(X)\n",
    "        return y_mean, y_cov\n",
    "    else:  # Predict based on GP posterior using kernel with optimized params\n",
    "        K_trans = self.kernel_(X, self.X_train_)\n",
    "        y_mean = K_trans.dot(self.alpha_)  # Line 4 (y_mean = f_star)\n",
    "#        y_mean = self._y_train_mean + y_mean  # undo normal.\n",
    "\n",
    "        v = np.linalg.solve(self.L_, K_trans.T)  # self.L_.T * K_inv * K_trans.T\n",
    "        y_cov = self.kernel_(X) - np.dot(v.T, v)  # Line 6\n",
    "        return y_mean, y_cov\n",
    "GP.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_y(self, X, n_samples=1):\n",
    "    y_mean, y_cov = self.predict(X)\n",
    "    try:\n",
    "        L = np.linalg.cholesky(y_cov)\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        e.args = (\"The kernel, %s, is not returning a\" \n",
    "                  \"positive definite matrix. Try gradually \"\n",
    "                 \"increasing the 'alpha' parameter of your \"\n",
    "                    \"GaussianProcessRegressor estimator.\"\n",
    "                  % self.kernel,) + e.args\n",
    "        raise\n",
    "    u = np.random.randn(X.shape[0], n_samples)\n",
    "    z = np.dot(L, u)\n",
    "    return z\n",
    "GP.sample_y = sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_marginal_likelihood(self, theta=None, eval_gradient=False):\n",
    "    \n",
    "    if theta is None:\n",
    "        assert hasattr(self, \"log_marginal_likelihood_value_\")\n",
    "        print('Using fitted theta')\n",
    "        return self.log_marginal_likelihood_value_\n",
    "    \n",
    "    kernel = copy.deepcopy(self.kernel)\n",
    "    kernel.set_theta(*theta)\n",
    "    \n",
    "    if eval_gradient:\n",
    "        K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
    "    else:\n",
    "        K = kernel(self.X_train_)\n",
    "        \n",
    "    # Compute log-likelihood (eqn. 2.30)\n",
    "    log_likelihood = -0.5 * np.einsum(\"i,i\", self.y_train_, self.alpha_)\n",
    "    log_likelihood -= np.log(np.diag(self.L_)).sum()\n",
    "    log_likelihood -= K.shape[0] / 2 * np.log(2 * np.pi)\n",
    "\n",
    "    if eval_gradient:\n",
    "        # Compute lml gradient w.r.t kernel params (eqn. 5.9)\n",
    "        tmp = np.einsum(\"i,j->ij\", self.alpha_, self.alpha_)\n",
    "        tmp -= np.linalg.solve(self.L_.T, np.linalg.solve(self.L_, np.eye(K.shape[0])))  # K inverse\n",
    "        log_likelihood_gradient = 0.5 * np.einsum(\"ij,ijk->k\", tmp, K_gradient)  # note K_gradient symmetric so ij same as ji. l output dim and k num params\n",
    "    \n",
    "    if eval_gradient:\n",
    "        return log_likelihood, log_likelihood_gradient\n",
    "    else:\n",
    "        return log_likelihood\n",
    "GP.log_marginal_likelihood = log_marginal_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.sort(np.random.uniform(-5, 5, 10))[:, np.newaxis]\n",
    "kernel = RBF(1, 1, 1e-10)\n",
    "gp = GP(kernel, 'L-BFGS-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior and take as observations\n",
    "y_sample = gp.sample_y(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, ax = plt.subplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a983550>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax.plot(X, y_sample, 'r+')\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(-5, 5, 0.005)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GP at 0x10a983438>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.fit(X, y_sample[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,   1.00000000e+00,   1.00000000e-10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.kernel_.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1e-10]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.kernel.theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All dimensions of input must be of equal length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-894367102b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-69ed181f57fe>\u001b[0m in \u001b[0;36msample_y\u001b[0;34m(self, X, n_samples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0my_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-344d81914284>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Predict based on GP posterior using kernel with optimized params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mK_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Line 4 (y_mean = f_star)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#        y_mean = self._y_train_mean + y_mean  # undo normal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-137212b2a37f>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X1, X2, eval_gradient)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Build total K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_variance\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_indices_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36mdiag_indices_from\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;31m# all dimensions equal, so we check first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malltrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All dimensions of input must be of equal length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdiag_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All dimensions of input must be of equal length"
     ]
    }
   ],
   "source": [
    "y_test = gp.sample_y(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
